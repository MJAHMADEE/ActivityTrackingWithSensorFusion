{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import integrate\n",
    "from scipy import interpolate\n",
    "import math\n",
    "import sys\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from numba import cuda\n",
    "import numba\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import display, HTML\n",
    "%run GRUSupportFns.ipynb\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.utils import multi_gpu_model\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, Bidirectional, LSTM\n",
    "from tensorflow.python.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.python.keras import backend as K \n",
    "from IPython.display import clear_output\n",
    "from tensorflow.python.keras.initializers import RandomUniform\n",
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DataStream and Ground Truth\n",
    "As no reliable ground truth currently, using Kalman Filter results. Note: Ground truth must currently have the same timesteps as the input.\n",
    "\n",
    "https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/23_Time-Series-Prediction.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Data uni1\n",
      "READ DATA FROM CACHE\n",
      "READ GROUND TRUTH DATA FROM CACHE\n",
      "Parsing Data uni\n",
      "READ DATA FROM CACHE\n",
      "READ GROUND TRUTH DATA FROM CACHE\n",
      "Parsing Data uni\n",
      "READ DATA FROM CACHE\n",
      "READ GROUND TRUTH DATA FROM CACHE\n",
      "Parsing Data data5\n",
      "READ DATA FROM CACHE\n",
      "READ GROUND TRUTH DATA FROM CACHE\n"
     ]
    }
   ],
   "source": [
    "training_files = ['uni1', 'uni']\n",
    "testing_files = ['uni', 'data5']\n",
    "\n",
    "training_dataset = load_datasets(training_files)\n",
    "testing_dataset = load_datasets(testing_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 270\n",
    "batch_size = 1024\n",
    "seq_offset = 2\n",
    "training_length = 0\n",
    "for activity in training_dataset:\n",
    "    training_length += len(activity[0])\n",
    "\n",
    "batches_per_epoch = int(training_length/batch_size + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# colors = matplotlib.cm.rainbow(np.linspace(0, 1, 10))\n",
    "\n",
    "# plt.figure(figsize=(9.5, 8))\n",
    "# ax=plt.subplot(111)\n",
    "# for i in range(0, len(training_dataset[0][0]), seq_len):\n",
    "#     plt.plot(training_dataset[0][0][i:i+seq_len, 0], training_dataset[0][0][i:i+seq_len, 1], '-', \n",
    "#              c=colors[int(i/seq_len) % 10])    \n",
    "#     plt.plot(training_dataset[0][1][i:i+seq_len, 0], training_dataset[0][1][i:i+seq_len, 1], '-', \n",
    "#              c=colors[int(i/seq_len) % 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_training_dataset, scaled_testing_dataset = scale_dataset(training_dataset, testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Validation Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179044, 270, 11)\n",
      "(51199, 270, 11)\n",
      "Batches per Epoch:  1\n",
      "(51199, 270, 11)\n"
     ]
    }
   ],
   "source": [
    "x_train_seqs, y_train_seqs = get_seqs(sequence_length=seq_len, dataset=scaled_training_dataset, offset=seq_offset)\n",
    "x_test_seqs, y_test_seqs = get_seqs(sequence_length=seq_len, dataset=scaled_testing_dataset, offset=seq_offset)\n",
    "print(x_train_seqs.shape)\n",
    "print(x_test_seqs.shape)\n",
    "\n",
    "\n",
    "val_batch_size = int(0.25*batches_per_epoch*batch_size)\n",
    "val_generator = batch_generator(batch_size=val_batch_size, x_seqs=x_test_seqs, y_seqs=y_test_seqs)\n",
    "val_batch_x, val_batch_y = next(val_generator)\n",
    "\n",
    "validation_data = (val_batch_x, val_batch_y)\n",
    "print(validation_data[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Tensorflow Session \n",
    "\n",
    "Batches in the shape (Batch Size, Sequence Length, Feature Vector Length)\n",
    "\n",
    "Hyper-parameters:\n",
    " - Batch Size\n",
    " - Sequence Length\n",
    " - Number of States in RNN layer\n",
    " - GRU or LSTM Layer\n",
    " - All forward facing layers, or bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "generator = batch_generator(batch_size=batch_size, x_seqs=x_train_seqs, y_seqs=y_train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(units=128, return_sequences=True, input_shape=(None, x_dim,)))\n",
    "model.add(GRU(units=128, return_sequences=True, input_shape=(None, x_dim,)))\n",
    "model.add(GRU(units=128, return_sequences=True, input_shape=(None, x_dim,)))\n",
    "model.add(GRU(units=128, return_sequences=True, input_shape=(None, x_dim,)))\n",
    "model.add(GRU(units=128, return_sequences=True, input_shape=(None, x_dim,)))\n",
    "model.add(GRU(units=128, return_sequences=True, input_shape=(None, x_dim,)))\n",
    "# model.add(GRU(units=1024, return_sequences=True))\n",
    "model.add(Dense(y_dim, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Bidirectional(GRU(units=128, return_sequences=True), merge_mode='ave',input_shape=(None, x_dim,)))\n",
    "# model.add(Bidirectional(GRU(units=128, return_sequences=True), merge_mode='ave',input_shape=(None, x_dim,)))\n",
    "# model.add(Bidirectional(GRU(units=128, return_sequences=True), merge_mode='ave',input_shape=(None, x_dim,)))\n",
    "# model.add(Dense(y_dim, activation='linear'))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, None, 128)         53760     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 128)         98688     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 128)         98688     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, None, 128)         98688     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, None, 128)         98688     \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, None, 128)         98688     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 2)           258       \n",
      "=================================================================\n",
      "Total params: 547,458\n",
      "Trainable params: 547,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr=1e-3)\n",
    "# model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "model.compile(loss=custom_loss, optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_checkpoint = '23_checkpoint.keras'\n",
    "# callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "#                                       monitor='val_loss',\n",
    "#                                       verbose=1,\n",
    "#                                       save_weights_only=True,\n",
    "#                                       save_best_only=True)\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=3, verbose=1)\n",
    "# callback_tensorboard = TensorBoard(log_dir='./23_logs/',\n",
    "#                                    histogram_freq=0,\n",
    "#                                    write_graph=False)\n",
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-7,\n",
    "                                       patience=0,\n",
    "                                       verbose=1)      \n",
    "plot_losses = PlotLosses()\n",
    "\n",
    "callbacks = [callback_early_stopping,\n",
    "#              callback_checkpoint,\n",
    "#              callback_tensorboard,\n",
    "             plot_losses,\n",
    "             callback_reduce_lr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Batches per Epoch:  175\n",
      " 86/351 [======>.......................] - ETA: 7:34 - loss: 38.7742"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# model.fit(np.expand_dims(scaled_training_data, axis=0), np.expand_dims(scaled_training_labels, axis=0), epochs=10, \n",
    "#           batch_size=32, validation_data=validation_data, callbacks=callbacks)\n",
    "model.fit_generator(generator=generator,\n",
    "                    epochs=30,\n",
    "                    steps_per_epoch=batches_per_epoch, #Number of batches per epoch\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activity in scaled_testing_dataset:\n",
    "    print(\"Test Data\")\n",
    "    result = model.evaluate(x=np.expand_dims(activity[0], axis=0),\n",
    "                            y=np.expand_dims(activity[1], axis=0))\n",
    "    print(\"loss (test-set):\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_dataset(training_dataset, seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    " - Choose the maximum amount of epochs\n",
    " - How many batches of the data per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_dataset(testing_dataset, seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
